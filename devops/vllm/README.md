# Fundamentos de vLLM: Virtual Large Language Model

Este reposit√≥rio explora os fundamentos do **vLLM**, uma biblioteca de infer√™ncia r√°pida e eficiente para Large Language Models (LLMs).  
O material baseia-se na apresenta√ß√£o **"Fundamentos de vLLM"**.

---

## üí° Motiva√ß√£o: Por Que vLLM?

Modelos de Linguagem Grandes (LLMs), como Llama e Gemini, est√£o cada vez maiores e mais complexos, o que amplia suas capacidades, mas traz desafios:

### Desafios com LLMs Grandes

- **Custo e Recursos de Hardware:** GPUs s√£o essenciais para acelerar infer√™ncia e treinamento devido aos c√°lculos matriciais intensivos. GPUs de alta performance (ex: NVIDIA A100 40GB) s√£o caras.  
- **Gera√ß√£o Autoregressiva:** Cada token √© gerado com base nos tokens anteriores, tornando a gera√ß√£o inerentemente serial e criando gargalos.  
- **KV Cache:** Para evitar rec√°lculos na aten√ß√£o a cada token gerado, utiliza-se um cache das matrizes Key (K) e Value (V). Por√©m, esse cache pode consumir mais de 30% da mem√≥ria da GPU e seu tamanho varia dinamicamente, o que dificulta abordagens tradicionais.  

### Processamento de Requisi√ß√µes

- **Enfileiramento (Queuing):** Pode criar gargalos e comprometer o tempo real de resposta, essencial em aplica√ß√µes como DALL-E.  
- **Batching de Requisi√ß√µes:** Processar v√°rias requisi√ß√µes em lote √© eficiente, mas complexo, pois as requisi√ß√µes chegam em momentos e tamanhos diferentes.  
- **Batching por Itera√ß√£o:** Permite iniciar processamento antes do lote estar completo, substituindo requisi√ß√µes conforme terminam.  

### Fragmenta√ß√£o de Mem√≥ria (M√©todos Tradicionais)

- Aloca√ß√£o cont√≠gua do espa√ßo m√°ximo da senten√ßa causa:  
  - **Fragmenta√ß√£o Interna:** Espa√ßo reservado mas n√£o utilizado.  
  - **Fragmenta√ß√£o Externa:** Espa√ßo n√£o reservado e n√£o utilizado entre blocos.  
- Resultado: grande inefici√™ncia no uso da mem√≥ria GPU.

---

## üöÄ PagedAttention

vLLM soluciona a inefici√™ncia do KV Cache com a t√©cnica **PagedAttention**, inspirada na pagina√ß√£o de mem√≥ria dos sistemas operacionais:

- **Blocos N√£o Cont√≠guos:** KV Cache √© dividido em blocos, cada um contendo K e V de um n√∫mero fixo de tokens, n√£o armazenados contiguamente na mem√≥ria.  
- **Aloca√ß√£o Din√¢mica:** Evita espa√ßos ociosos e reduz fragmenta√ß√£o interna e externa.  
- **Compartilhamento de KV Cache:** Permite que diferentes requisi√ß√µes compartilhem KV Cache, especialmente √∫til com prompts comuns.  
- **Otimiza√ß√µes em Baixo N√≠vel:** Kernels otimizados aceleram transfer√™ncia de dados e infer√™ncia.  

**Analogia:**  
Blocos = P√°ginas, Tokens = Bytes, Requisi√ß√µes = Processos.

---

## üì¶ Arquitetura do vLLM

- Kernels otimizados com PagedAttention para gerenciamento eficiente do KV Cache.  
- API de infer√™ncia para intera√ß√£o com o modelo.  
- Schedulers e blocos para gest√£o eficiente de requisi√ß√µes e aloca√ß√£o de mem√≥ria.

---

## üíª Parte Pr√°tica: Configurando e Servindo um Modelo com vLLM (CPU)

Este guia mostra como configurar e executar um modelo vLLM localmente via CPU usando Docker.

### Pr√©-requisitos

- **Git:** Para clonar o reposit√≥rio oficial.  
- **Docker Desktop ou Docker Engine:** Para construir e executar o container. Certifique-se que o Docker est√° rodando.

### Passos

1. **Clone o reposit√≥rio vLLM:**

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
````

2. **Construa a imagem Docker para CPU:**

```bash
docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .
```

> O par√¢metro `--shm-size=4g` aloca mem√≥ria compartilhada suficiente para o modelo. Esse processo pode levar alguns minutos.

3. **Execute o container para servir o modelo:**

```bash
docker run -it \
  --rm \
  --network=host \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  vllm-cpu-env \
  --model Qwen/Qwen2-1.5B-Instruct \
  --trust-remote-code \
  --device cpu \
  --dtype bfloat16 \
  --tokenizer-mode auto
```

### Explica√ß√£o dos par√¢metros do `docker run`

* `-it`: modo interativo com pseudo-TTY.
* `--rm`: remove o container automaticamente ao finalizar.
* `--network=host`: conecta o container √† rede do host (localhost).
* `-v ~/.cache/huggingface:/root/.cache/huggingface`: monta o cache do Hugging Face para persist√™ncia de modelos e acelera downloads futuros.
* `vllm-cpu-env`: nome da imagem Docker constru√≠da.
* `--model Qwen/Qwen2-1.5B-Instruct`: modelo a ser carregado (pode substituir por outros dispon√≠veis).
* `--trust-remote-code`: permite executar c√≥digo remoto necess√°rio em alguns modelos.
* `--device cpu`: usa CPU para infer√™ncia.
* `--dtype bfloat16`: define tipo de dado para pesos (comum em CPU).
* `--tokenizer-mode auto`: modo autom√°tico para carregar tokenizador.

Ao iniciar, o servidor vLLM estar√° pronto para receber requisi√ß√µes.

---

## üì® Fazendo uma Requisi√ß√£o (Exemplo com cURL)

Com o servidor rodando, abra outro terminal e envie uma requisi√ß√£o POST para a API:

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2-1.5B-Instruct",
    "prompt": "Explique a import√¢ncia da IA.",
    "max_tokens": 50,
    "temperature": 0.7
  }'
```

Voc√™ receber√° uma resposta JSON com o texto gerado pelo modelo.

---

## üìö Refer√™ncias

## Refer√™ncias

* [vLLM Project GitHub](https://github.com/vllm-project/vllm)
* [vLLM Documentation: CPU Installation](https://docs.vllm.ai/en/v0.7.3/getting_started/installation/cpu/index.html)
* [Efficient Memory Management for Large Language
Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180)
* [SO-CM Slides (UFPR)](https://wiki.inf.ufpr.br/maziero/lib/exe/fetch.php?media=socm%3Asocm-slides-17.pdf)
* [Efficient Generative Large Language Model Serving ‚Äì Medium](https://medium.com/@javaid.nabi/efficient-generative-large-language-model-serving-1c22b58f3c92)
* [Gemini 1.5 Pro vs GPT-4o: A Head-to-Head Showdown ‚Äì Medium](https://medium.com/@neltac33/gemini-1-5-pro-vs-gpt-4o-a-head-to-head-showdown-29c4cc837e7b)
* [How ChatGPT Works ‚Äì Wired](https://www.wired.com/story/how-chatgpt-works-large-language-model/?utm_source=chatgpt.com)
